{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VDCNN Implementation In Torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import pyprind\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = r'''abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’\"/|$%ˆ&*˜‘+=<>()[]{}'''\n",
    "# nltk.regexp_tokenize(\"I love,  this ship\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    s = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’\"/|$%ˆ&*˜‘+=<>()[]{} '\n",
    "    return [l for l in list(text.lower()) if l in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenize, fix_length=1014)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pos': 12500, 'neg': 12500})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEXT.vocab.stoi\n",
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neo/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GT 755M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    te, y = batch.text, batch.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlockRes(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, shortcut=False, pool_type=\"max_pool\"):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.conv_1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if shortcut is True:\n",
    "            self.conv_res = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2)\n",
    "            self.batch_norm_res = nn.BatchNorm1d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(self.batch_norm_1(out))\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(self.batch_norm_2(out))\n",
    "        \n",
    "        # downsampled\n",
    "        if self.pool_type == \"k_max\":\n",
    "            k_ = math.ceil(out.shape[2]/2.0)\n",
    "            out = downsample_k_max_pool(out, k=k_, dim=2)[0]\n",
    "        else:\n",
    "            out = downsample_max_pool(out, 3, 2)\n",
    "\n",
    "        if self.shortcut is True:\n",
    "            residual = self.conv_res(x)\n",
    "            residual = F.relu(self.batch_norm_res(residual))\n",
    "            out = out + residual\n",
    "        return out\n",
    "\n",
    "class ConvolutionalIdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, padding=1, shortcut=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shortcut = shortcut\n",
    "        self.conv_1 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(in_channels)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(self.batch_norm_1(out))\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(self.batch_norm_2(out))\n",
    "\n",
    "        if self.shortcut is True:\n",
    "            out = out + x\n",
    "        else:\n",
    "            out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "def downsample_max_pool(x, kernel_size, stride):\n",
    "    pool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=1)\n",
    "    return pool(x)\n",
    "\n",
    "\n",
    "def downsample_k_max_pool(inp, k, dim):\n",
    "    return inp.topk(k, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocab_size)\n",
    "        \n",
    "        self.conv_64 = nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.id_64 = ConvolutionalIdentityBlock(64, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "        self.res_128 = ConvolutionalBlockRes(in_channels=64, out_channels=128, kernel_size=3, padding=1, shortcut=True, pool_type=\"k_max\")\n",
    "        \n",
    "        self.id_128 = ConvolutionalIdentityBlock(128, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "        self.res_256 = ConvolutionalBlockRes(in_channels=128, out_channels=256, kernel_size=3, padding=1, shortcut=True, pool_type=\"k_max\")\n",
    "\n",
    "        self.id_256 = ConvolutionalIdentityBlock(256, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "        self.res_512 = ConvolutionalBlockRes(in_channels=256, out_channels=512, kernel_size=3, padding=1, shortcut=True, pool_type=\"k_max\")\n",
    "        \n",
    "        self.id_512 = ConvolutionalIdentityBlock(512, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "        self.linear_1 = nn.Linear(8*512, 2048)\n",
    "        self.linear_2 = nn.Linear(2048, 2048)\n",
    "        self.linear_3 = nn.Linear(2048, n_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # [sent_length, batch_size]\n",
    "        inp = inp.permute(1, 0)\n",
    "        \n",
    "        # [batch_size, sent_length]\n",
    "        embedded = self.embedding(inp)\n",
    "        \n",
    "        # [batch_size, sent_lenght, emb_dim]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # [batch_size, emb_dim, sent_length]\n",
    "        out = self.conv_64(embedded)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, 64, sent_length]\n",
    "        out = self.id_64(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 64, sent_length]\n",
    "        out = self.res_128(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 128, sent_length/2]\n",
    "        out = self.id_128(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 128, sent_length/2]\n",
    "        out = self.res_256(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 256, sent_length/4]\n",
    "        out = self.id_256(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 256, sent_length/4]\n",
    "        out = self.res_512(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 512, sent_length/8]\n",
    "        out = self.id_512(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 512, sent_length/8]\n",
    "        out = downsample_k_max_pool(out, k=8, dim=2)[0]\n",
    "#         return k_max_pooled\n",
    "#         print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "#         print(out.shape)\n",
    "        # [batch_size, 512, 8]\n",
    "        out = self.linear_1(out)\n",
    "        \n",
    "        # [batch_size, 4096]\n",
    "        out = self.linear_2(out)\n",
    "#         print(out.shape)\n",
    "\n",
    "        # [batch_size, 512, 2048\n",
    "        out = self.linear_3(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, n_class]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f1a60eb04c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# a.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a = a.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "a = model(te)\n",
    "# a.shape\n",
    "\n",
    "# a = a.squeeze(1)\n",
    "# a\n",
    "\n",
    "# a.shape\n",
    "\n",
    "# y\n",
    "\n",
    "# preds = torch.round(torch.sigmoid(a))\n",
    "# preds\n",
    "# #     correct = (preds == y).float()\n",
    "# #     acc = correct.sum()/float(len(correct))\n",
    "\n",
    "# correct = (preds == y).float()\n",
    "# correct.sum()/len(correct)\n",
    "\n",
    "# binary_accuracy(a, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "vocab_size = len(TEXT.vocab.stoi)\n",
    "n_classes = 1\n",
    "\n",
    "model = VDCNN(embedding_dim, vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VDCNN(\n",
      "  (embedding): Embedding(65, 16)\n",
      "  (conv_64): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (id_64): ConvolutionalIdentityBlock(\n",
      "    (conv_1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (res_128): ConvolutionalBlockRes(\n",
      "    (conv_1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_res): Conv1d(64, 128, kernel_size=(1,), stride=(2,))\n",
      "    (batch_norm_res): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (id_128): ConvolutionalIdentityBlock(\n",
      "    (conv_1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (res_256): ConvolutionalBlockRes(\n",
      "    (conv_1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_res): Conv1d(128, 256, kernel_size=(1,), stride=(2,))\n",
      "    (batch_norm_res): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (id_256): ConvolutionalIdentityBlock(\n",
      "    (conv_1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (res_512): ConvolutionalBlockRes(\n",
      "    (conv_1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_res): Conv1d(256, 512, kernel_size=(1,), stride=(2,))\n",
      "    (batch_norm_res): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (id_512): ConvolutionalIdentityBlock(\n",
      "    (conv_1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_1): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (linear_3): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#     bar = pyprind.ProgBar(100, bar_char='█')\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "#         print(predictions.shape, batch.Label.shape, model(batch.Text).shape)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "#         print(loss.shape)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        bar.update()\n",
    "        \n",
    "#         if i > 100:\n",
    "#             break\n",
    "#         break\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#         bar = pyprind.ProgBar(100, bar_char='█')\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            bar.update()\n",
    "#             if i > 100:\n",
    "#                 break\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te.shape\n",
    "\n",
    "# out = model(te).squeeze(1)\n",
    "# out.shape\n",
    "\n",
    "# loss = criterion(out, y)\n",
    "# loss.shape\n",
    "\n",
    "# loss\n",
    "\n",
    "# sig = torch.sigmoid(out)\n",
    "# sig\n",
    "\n",
    "# preds = torch.round(sig)\n",
    "# preds.shape\n",
    "\n",
    "# preds\n",
    "\n",
    "# correct = (preds == y).float()\n",
    "# correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:33\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.745 | Train Acc: 50.25% | Val. Loss: 0.692 | Val. Acc: 51.19% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:27\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 02 | Train Loss: 0.592 | Train Acc: 65.65% | Val. Loss: 0.679 | Val. Acc: 69.44% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:33\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:28\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 03 | Train Loss: 0.426 | Train Acc: 80.20% | Val. Loss: 0.777 | Val. Acc: 66.37% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:34\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:28\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 04 | Train Loss: 0.384 | Train Acc: 82.83% | Val. Loss: 0.480 | Val. Acc: 77.60% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:35\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:29\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 05 | Train Loss: 0.351 | Train Acc: 84.64% | Val. Loss: 0.400 | Val. Acc: 81.86% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:34\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:29\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 06 | Train Loss: 0.337 | Train Acc: 85.43% | Val. Loss: 0.371 | Val. Acc: 83.31% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:29\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 07 | Train Loss: 0.312 | Train Acc: 86.55% | Val. Loss: 0.370 | Val. Acc: 83.40% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:35\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:29\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 08 | Train Loss: 0.297 | Train Acc: 87.42% | Val. Loss: 0.388 | Val. Acc: 82.45% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:34\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:30\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 09 | Train Loss: 0.282 | Train Acc: 88.23% | Val. Loss: 0.378 | Val. Acc: 83.47% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:35\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:30\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 10 | Train Loss: 0.270 | Train Acc: 88.87% | Val. Loss: 0.541 | Val. Acc: 75.53% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 11 | Train Loss: 0.252 | Train Acc: 89.54% | Val. Loss: 0.371 | Val. Acc: 83.08% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:30\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 12 | Train Loss: 0.243 | Train Acc: 90.15% | Val. Loss: 0.399 | Val. Acc: 82.74% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 13 | Train Loss: 0.229 | Train Acc: 90.63% | Val. Loss: 0.410 | Val. Acc: 83.07% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:30\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 14 | Train Loss: 0.212 | Train Acc: 91.50% | Val. Loss: 0.432 | Val. Acc: 82.35% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:30\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 15 | Train Loss: 0.201 | Train Acc: 91.95% | Val. Loss: 0.439 | Val. Acc: 82.90% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:35\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 16 | Train Loss: 0.191 | Train Acc: 92.29% | Val. Loss: 0.473 | Val. Acc: 82.05% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:36\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 17 | Train Loss: 0.179 | Train Acc: 92.91% | Val. Loss: 0.571 | Val. Acc: 79.75% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:37\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 18 | Train Loss: 0.172 | Train Acc: 93.20% | Val. Loss: 0.434 | Val. Acc: 82.11% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:37\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 19 | Train Loss: 0.157 | Train Acc: 93.75% | Val. Loss: 0.546 | Val. Acc: 83.01% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:38\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 20 | Train Loss: 0.151 | Train Acc: 94.02% | Val. Loss: 0.541 | Val. Acc: 80.87% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:35\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:31\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 21 | Train Loss: 0.139 | Train Acc: 94.64% | Val. Loss: 0.579 | Val. Acc: 83.00% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:37\n",
      "0% [██                            ] 100% | ETA: 00:12:41"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = tokenize(sentence)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor).squeeze(1)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.round(torch.sigmoid(tensor))\n",
    "#     print(preds)\n",
    "    return preds, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"My voice range is A2-C5. My chest voice goes up to F4. Included sample in my higher chest range. What is my voice type?\"\n",
    "# predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
