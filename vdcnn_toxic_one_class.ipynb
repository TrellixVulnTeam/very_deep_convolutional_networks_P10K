{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VDCNN Implementation In Torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import pyprind\n",
    "import math\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    s = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’\"/|_#$%ˆ&*˜‘+=<>()[]{} '\n",
    "    return [l for l in list(text.lower()) if l in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = data.Field(\n",
    "    sequential=True,\n",
    "    use_vocab=True,\n",
    "#     init_token=\"<ios>\",\n",
    "#     eos_token=\"<eos>\",\n",
    "    fix_length=SENT_LENGTH,\n",
    "    tokenize=tokenizer,\n",
    "    batch_first=True\n",
    ")\n",
    "label_field = data.Field(\n",
    "    sequential=False,\n",
    "    use_vocab=False,\n",
    "    is_target=True,\n",
    "    dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fields = [\n",
    "    (\"id\", None),\n",
    "    (\"comment_text\", text_field),\n",
    "    (\"toxic\", label_field),\n",
    "    (\"severe_toxic\", None), (\"threat\", None),\n",
    "    (\"obscene\", None), (\"insult\", None),\n",
    "    (\"identity_hate\", None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds, valds = data.TabularDataset.splits(\n",
    "    path=\"data/toxic_competition_data/\",\n",
    "    format=\"csv\",\n",
    "    train=\"train_torch.csv\",\n",
    "    validation=\"test_torch.csv\",\n",
    "    fields=csv_fields,\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143613, 15958)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainds), len(valds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainds.examples[1].toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence/phrase ----------------- Label\n",
      "hey isambard, thanks comment. see section oed, merriam-webster dictionary.com less agree definition. right cite article - ill that. thanks again. ------------- 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence/phrase ----------------- Label\")\n",
    "for i in range(10):\n",
    "    print(\"{} ------------- {}\".format(\"\".join(trainds.examples[i].comment_text), trainds.examples[i].toxic))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(trainds)\n",
    "label_field.build_vocab(trainds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 5179297),\n",
       " ('e', 3978663),\n",
       " ('i', 2621386),\n",
       " ('a', 2491486),\n",
       " ('t', 2362920),\n",
       " ('s', 2196218),\n",
       " ('n', 2164892),\n",
       " ('o', 2040191),\n",
       " ('r', 2016075),\n",
       " ('l', 1646713)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2244, 250)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "traindl, valdl = data.BucketIterator.splits(\n",
    "    datasets=(trainds, valds),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE),\n",
    "    sort_key= lambda x: x.comment_text,\n",
    "    repeat=False,\n",
    "    device=device\n",
    ")\n",
    "len(traindl), len(valdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neo/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GT 755M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.comment_text]:[torch.cuda.LongTensor of size 64x1024 (GPU 0)]\n",
       "\t[.toxic]:[torch.cuda.FloatTensor of size 64 (GPU 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(traindl.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            y = getattr(batch, self.y_vars)\n",
    "#             if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "#                 y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "#             else:\n",
    "#                 y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(traindl, \"comment_text\", \"toxic\")\n",
    "valid_dl = BatchWrapper(valdl, \"comment_text\", \"toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[18,  5, 18,  ...,  1,  1,  1],\n",
       "         [18,  4,  2,  ...,  1,  1,  1],\n",
       "         [35,  4, 17,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [16,  9, 30,  ...,  1,  1,  1],\n",
       "         [ 8,  9,  6,  ...,  1,  1,  1],\n",
       "         [ 5, 10,  6,  ...,  1,  1,  1]], device='cuda:0'),\n",
       " tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlockRes(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, shortcut=False, pool_type=\"max_pool\"):\n",
    "        super().__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.conv_2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if shortcut is True:\n",
    "            self.conv_res = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2)\n",
    "            self.batch_norm_res = nn.BatchNorm1d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_1(x)\n",
    "        out_1 = F.relu(self.batch_norm_1(out_1))\n",
    "\n",
    "        out = self.conv_2(out_1)\n",
    "        out = F.relu(self.batch_norm_2(out))\n",
    "\n",
    "        if self.shortcut is False:\n",
    "            return out\n",
    "        else:\n",
    "            residual = self.conv_res(x)\n",
    "            residual = F.relu(self.batch_norm_res(residual))\n",
    "            if self.pool_type == \"k_max\":\n",
    "                k_ = math.ceil(out.shape[2]/2.0)\n",
    "                downsampled = downsample_k_max_pool(out, k=k_, dim=2)[0]\n",
    "            else:\n",
    "                downsampled = downsample_max_pool(out, 3, 2)\n",
    "            out = downsampled + residual\n",
    "            return out\n",
    "\n",
    "class ConvolutionalIdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, padding=1, shortcut=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shortcut = shortcut\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(in_channels)\n",
    "        \n",
    "        self.conv_2 = nn.Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv_1(x)\n",
    "        out_1 = F.relu(self.batch_norm_1(out_1))\n",
    "\n",
    "        out = self.conv_2(out_1)\n",
    "        out = F.relu(self.batch_norm_2(out))\n",
    "\n",
    "        if self.shortcut is True:\n",
    "            out = out + x\n",
    "        else:\n",
    "            out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "def downsample_max_pool(x, kernel_size, stride):\n",
    "    pool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=1)\n",
    "    return pool(x)\n",
    "\n",
    "\n",
    "def downsample_k_max_pool(inp, k, dim):\n",
    "    return inp.topk(k, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(embedding_dim=embedding_dim, num_embeddings=vocab_size)\n",
    "        \n",
    "        self.conv_64 = nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.id_64 = ConvolutionalIdentityBlock(64, kernel_size=3, padding=1, shortcut=False)\n",
    "        \n",
    "#         self.res_128 = ConvolutionalBlockRes(in_channels=64, out_channels=128, kernel_size=3, padding=1, shortcut=False, pool_type=\"max_pool\")\n",
    "        \n",
    "#         self.id_128 = ConvolutionalIdentityBlock(128, kernel_size=3, padding=1, shortcut=False)\n",
    "        \n",
    "#         self.res_256 = ConvolutionalBlockRes(in_channels=128, out_channels=256, kernel_size=3, padding=1, shortcut=True, pool_type=\"max_pool\")\n",
    "\n",
    "#         self.id_256 = ConvolutionalIdentityBlock(256, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "#         self.res_512 = ConvolutionalBlockRes(in_channels=256, out_channels=512, kernel_size=3, padding=1, shortcut=True, pool_type=\"max_pool\")\n",
    "        \n",
    "#         self.id_512 = ConvolutionalIdentityBlock(512, kernel_size=3, padding=1, shortcut=True)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.linear_1 = nn.Linear(3*64, 2048)\n",
    "        self.linear_2 = nn.Linear(2048, 2048)\n",
    "        self.linear_3 = nn.Linear(2048, n_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # [batch_size, sent_length]\n",
    "        embedded = self.embedding(inp)\n",
    "#         print(embedded.shape)\n",
    "        \n",
    "        # [batch_size, sent_lenght, emb_dim]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "#         print(embedded.shape)\n",
    "        \n",
    "        # [batch_size, emb_dim, sent_length]\n",
    "        out = self.conv_64(embedded)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, 64, sent_length]\n",
    "        out = self.id_64(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 64, sent_length]\n",
    "#         out = self.res_128(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 128, sent_length/2]\n",
    "#         out = self.id_128(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 128, sent_length/2]\n",
    "#         out = self.res_256(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 256, sent_length/4]\n",
    "#         out = self.id_256(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 256, sent_length/4]\n",
    "#         out = self.res_512(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 512, sent_length/8]\n",
    "#         out = self.id_512(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "#         # [batch_size, 512, sent_length/8]\n",
    "        out = downsample_k_max_pool(out, k=3, dim=2)[0]\n",
    "#         return k_max_pooled\n",
    "#         print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "#         print(out.shape)\n",
    "        # [batch_size, 512, 8]\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        # [batch_size, 4096]\n",
    "        out = F.relu(self.linear_2(out))\n",
    "#         print(out.shape)\n",
    "        out = self.drop2(out)\n",
    "\n",
    "        # [batch_size, 512, 2048\n",
    "        out = self.linear_3(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, n_class]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "vocab_size = len(text_field.vocab.stoi)\n",
    "n_classes = 1\n",
    "\n",
    "model = VDCNN(embedding_dim, vocab_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model are : 4622849\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Number of trainable parameters in the model are : {}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VDCNN(\n",
      "  (embedding): Embedding(68, 16)\n",
      "  (conv_64): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (id_64): ConvolutionalIdentityBlock(\n",
      "    (conv_1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop1): Dropout(p=0.3)\n",
      "  (drop2): Dropout(p=0.3)\n",
      "  (linear_1): Linear(in_features=192, out_features=2048, bias=True)\n",
      "  (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (linear_3): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc\n",
    "\n",
    "def roc_auc_score_FIXED(y_true, y_pred):\n",
    "    if len(np.unique(y_true)) == 1: # bug in roc_auc_score\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def get_avg_roc_value(y, output):\n",
    "    out = torch.sigmoid(output)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    y = y.cpu().detach().numpy()\n",
    "    \n",
    "    roc = roc_auc_score_FIXED(y, out)\n",
    "    return roc\n",
    "\n",
    "def get_avg_roc_value_2(y_fin, output_fin):\n",
    "    n = len(y_fin)\n",
    "    out_list = []\n",
    "    y_list = []\n",
    "    for i in range(n):\n",
    "        out_list.extend(list(output_fin[i]))\n",
    "        y_list.extend(list(y_fin[i]))\n",
    "            \n",
    "    roc = roc_auc_score_FIXED(y_list, out_list)\n",
    "\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_roc = 0\n",
    "    all_y = []\n",
    "    all_out_list = []\n",
    "    \n",
    "    model.train()\n",
    "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x).squeeze(1)\n",
    "        loss = criterion(outputs, y)\n",
    "        acc = binary_accuracy(outputs, y)\n",
    "        roc = get_avg_roc_value(y, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_roc += roc\n",
    "        \n",
    "        all_out_list.append(torch.sigmoid(outputs).cpu().detach().numpy())\n",
    "        all_y.append(y.cpu().detach().numpy())\n",
    "        \n",
    "        bar.update()\n",
    "    roc_main = get_avg_roc_value_2(all_y, all_out_list)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_roc / len(iterator), roc_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_roc = 0\n",
    "    \n",
    "    all_y = []\n",
    "    all_out_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "        for x, y in iterator:\n",
    "            outputs = model(x).squeeze(1)\n",
    "            loss = criterion(outputs, y)\n",
    "            acc = binary_accuracy(outputs, y)\n",
    "            roc = get_avg_roc_value(y, outputs)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_roc += roc\n",
    "            \n",
    "            all_out_list.append(torch.sigmoid(outputs).cpu().detach().numpy())\n",
    "            all_y.append(y.cpu().detach().numpy())\n",
    "            \n",
    "            bar.update()\n",
    "    roc_main = get_avg_roc_value_2(all_y, all_out_list)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_roc / len(iterator), roc_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"data/vdcnn_just_toxic_model_128.tar\"\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation roc did not improve\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:10:33\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.326 | Train ROC: 64.00 | Train Acc: 90.32%\n",
      "| Epoch: 01 | Val. Loss: 0.293 | Val. ROC: 67.30 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 62.14 | Val. Main ROC: 73.17 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 02 | Train Loss: 0.289 | Train ROC: 72.66 | Train Acc: 90.34%\n",
      "| Epoch: 02 | Val. Loss: 0.278 | Val. ROC: 69.46 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 71.73 | Val. Main ROC: 75.21 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 03 | Train Loss: 0.284 | Train ROC: 74.16 | Train Acc: 90.32%\n",
      "| Epoch: 03 | Val. Loss: 0.275 | Val. ROC: 70.45 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 73.40 | Val. Main ROC: 76.93 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 04 | Train Loss: 0.281 | Train ROC: 75.61 | Train Acc: 90.28%\n",
      "| Epoch: 04 | Val. Loss: 0.268 | Val. ROC: 70.12 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 74.54 | Val. Main ROC: 77.25 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 05 | Train Loss: 0.279 | Train ROC: 75.51 | Train Acc: 90.29%\n",
      "| Epoch: 05 | Val. Loss: 0.273 | Val. ROC: 67.98 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 74.75 | Val. Main ROC: 75.65 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 06 | Train Loss: 0.279 | Train ROC: 74.81 | Train Acc: 90.32%\n",
      "| Epoch: 06 | Val. Loss: 0.274 | Val. ROC: 70.13 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 74.42 | Val. Main ROC: 76.90 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 07 | Train Loss: 0.279 | Train ROC: 74.99 | Train Acc: 90.31%\n",
      "| Epoch: 07 | Val. Loss: 0.270 | Val. ROC: 70.89 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 74.37 | Val. Main ROC: 77.33 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 08 | Train Loss: 0.278 | Train ROC: 75.34 | Train Acc: 90.37%\n",
      "| Epoch: 08 | Val. Loss: 0.279 | Val. ROC: 70.46 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 74.86 | Val. Main ROC: 76.77 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 09 | Train Loss: 0.277 | Train ROC: 75.45 | Train Acc: 90.35%\n",
      "| Epoch: 09 | Val. Loss: 0.282 | Val. ROC: 71.61 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 75.02 | Val. Main ROC: 77.96 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 10 | Train Loss: 0.276 | Train ROC: 75.99 | Train Acc: 90.39%\n",
      "| Epoch: 10 | Val. Loss: 0.267 | Val. ROC: 71.17 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 75.31 | Val. Main ROC: 77.79 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 11 | Train Loss: 0.275 | Train ROC: 75.60 | Train Acc: 90.39%\n",
      "| Epoch: 11 | Val. Loss: 0.270 | Val. ROC: 71.97 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 75.26 | Val. Main ROC: 78.39 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 12 | Train Loss: 0.274 | Train ROC: 76.32 | Train Acc: 90.45%\n",
      "| Epoch: 12 | Val. Loss: 0.265 | Val. ROC: 70.67 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 75.88 | Val. Main ROC: 78.11 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 13 | Train Loss: 0.273 | Train ROC: 76.34 | Train Acc: 90.54%\n",
      "| Epoch: 13 | Val. Loss: 0.262 | Val. ROC: 70.27 | Val. Acc: 90.81% |\n",
      "| Train Main ROC: 75.74 | Val. Main ROC: 77.73 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 14 | Train Loss: 0.272 | Train ROC: 76.54 | Train Acc: 90.56%\n",
      "| Epoch: 14 | Val. Loss: 0.260 | Val. ROC: 72.62 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 76.01 | Val. Main ROC: 79.18 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 15 | Train Loss: 0.270 | Train ROC: 76.67 | Train Acc: 90.66%\n",
      "| Epoch: 15 | Val. Loss: 0.266 | Val. ROC: 69.82 | Val. Acc: 90.97% |\n",
      "| Train Main ROC: 76.29 | Val. Main ROC: 77.60 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 16 | Train Loss: 0.270 | Train ROC: 77.05 | Train Acc: 90.63%\n",
      "| Epoch: 16 | Val. Loss: 0.265 | Val. ROC: 69.55 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 76.43 | Val. Main ROC: 77.22 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 17 | Train Loss: 0.269 | Train ROC: 77.65 | Train Acc: 90.70%\n",
      "| Epoch: 17 | Val. Loss: 0.266 | Val. ROC: 71.52 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 76.98 | Val. Main ROC: 78.36 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 18 | Train Loss: 0.270 | Train ROC: 76.67 | Train Acc: 90.72%\n",
      "| Epoch: 18 | Val. Loss: 0.265 | Val. ROC: 70.01 | Val. Acc: 90.96% |\n",
      "| Train Main ROC: 76.16 | Val. Main ROC: 77.27 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 19 | Train Loss: 0.268 | Train ROC: 77.58 | Train Acc: 90.80%\n",
      "| Epoch: 19 | Val. Loss: 0.272 | Val. ROC: 70.29 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 76.75 | Val. Main ROC: 77.81 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 20 | Train Loss: 0.267 | Train ROC: 78.22 | Train Acc: 90.76%\n",
      "| Epoch: 20 | Val. Loss: 0.269 | Val. ROC: 72.52 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 77.51 | Val. Main ROC: 79.32 \n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "base_dev_roc = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc, train_roc, train_roc_main = train(model, train_dl, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_roc, valid_roc_main = evaluate(model, valid_dl, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train ROC: {train_roc*100:.2f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'| Epoch: {epoch+1:02} | Val. Loss: {valid_loss:.3f} | Val. ROC: {valid_roc*100:.2f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    print(f'| Train Main ROC: {train_roc_main*100:.2f} | Val. Main ROC: {valid_roc_main*100:.2f} ')\n",
    "    is_best = False\n",
    "    if base_dev_roc < valid_roc_main:\n",
    "        is_best = True,\n",
    "        base_dev_roc = valid_roc_main\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': valid_loss,\n",
    "        'best_dev_accuracy': valid_acc\n",
    "    }, is_best, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:17\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.266 | Train ROC: 77.98 | Train Acc: 90.81%\n",
      "| Epoch: 01 | Val. Loss: 0.262 | Val. ROC: 72.08 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 77.41 | Val. Main ROC: 79.04 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:06\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 02 | Train Loss: 0.265 | Train ROC: 78.25 | Train Acc: 90.85%\n",
      "| Epoch: 02 | Val. Loss: 0.266 | Val. ROC: 72.65 | Val. Acc: 90.58% |\n",
      "| Train Main ROC: 77.86 | Val. Main ROC: 79.32 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:43\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:07\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 03 | Train Loss: 0.265 | Train ROC: 78.10 | Train Acc: 90.92%\n",
      "| Epoch: 03 | Val. Loss: 0.257 | Val. ROC: 70.41 | Val. Acc: 91.11% |\n",
      "| Train Main ROC: 77.41 | Val. Main ROC: 78.14 \n",
      "=> Validation roc did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:45\n",
      "0% [███████                       ] 100% | ETA: 00:08:31"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c82e80a46510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_roc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_roc_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_roc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_roc_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-57280eb63a42>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_roc_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "base_dev_roc = 0.7932\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc, train_roc, train_roc_main = train(model, train_dl, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_roc, valid_roc_main = evaluate(model, valid_dl, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train ROC: {train_roc*100:.2f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'| Epoch: {epoch+1:02} | Val. Loss: {valid_loss:.3f} | Val. ROC: {valid_roc*100:.2f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    print(f'| Train Main ROC: {train_roc_main*100:.2f} | Val. Main ROC: {valid_roc_main*100:.2f} ')\n",
    "    is_best = False\n",
    "    if base_dev_roc < valid_roc_main:\n",
    "        is_best = True,\n",
    "        base_dev_roc = valid_roc_main\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': valid_loss,\n",
    "        'best_dev_accuracy': valid_acc\n",
    "    }, is_best, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = tokenize(sentence)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    \n",
    "    tensor = tensor.unsqueeze(1)\n",
    "#     print(tensor.shape)\n",
    "    prediction = model(tensor).squeeze(1)\n",
    "#     print(prediction)\n",
    "    preds, ind= torch.round(torch.sigmoid(tensor))\n",
    "#     print(preds)\n",
    "    return preds, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"My voice range is A2-C5. My chest voice goes up to F4. Included sample in my higher chest range. What is my voice type?\"\n",
    "# predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
